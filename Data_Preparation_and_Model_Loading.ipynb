{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quanticedu/llm-fine-tuning/blob/main/Data_Preparation_and_Model_Loading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation and Model Loading\n",
        "\n",
        "In this Colab notebook, you'll work to get your dataset ready for model fine-tuning. You'll also look at options for quantizing model parameters to match your training resources (i.e., GPU memory) and performance requirements.\n",
        "\n",
        "> This notebook is based on [@maximelabonne's LLama2 fine-tuning notebook](https://github.com/mlabonne/llm-course/blob/main/Fine_tune_Llama_2_in_Google_Colab.ipynb), which is, in turn, based on Younes Belkada's [GitHub Gist](https://gist.github.com/younesbelkada/9f7f75c94bdc1981c8ca5cc937d4a4da). It also borrows from [this example](https://github.com/brevdev/notebooks/blob/main/phi2-finetune-own-data.ipynb) on phi2 fine-tuning.\n",
        "\n",
        "Note: in this course we're *not* going to cover every line of code. Much of the code is well-commented, and from the comments and the online documentation you should be able to figure out how any particular piece of code works.\n"
      ],
      "metadata": {
        "id": "OSHlAbqzDFDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Python Libraries\n",
        "\n",
        "First, we need to install the necessary libraries. We've included specific versions known to work for all packages involved. Few things are more frustrating than when old code that used to run smoothly and give desired results fails to run because of package version updates and resulting incompatibilities."
      ],
      "metadata": {
        "id": "Yxoj7Ey2QJEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate==0.26.1 bitsandbytes==0.42.0 datasets==2.14.6 peft==0.7.1 transformers==4.36.2 torch==2.1.0 einops==0.4.1 trl==0.7.0\n",
        "\n",
        "import torch\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, # Will be used to load the pre-trained model\n",
        "    AutoTokenizer, # Will be used to load the pre-trained tokenizer\n",
        "    BitsAndBytesConfig, # For model quantization settings\n",
        "    GenerationConfig, # To control generation (inference) from a model\n",
        "    TrainingArguments, # To specify parameters of the fine-tuning process\n",
        "    Trainer, # The object that abstracts away the training and evaluation loop\n",
        "    pipeline, # Stringing together tokenization and inference, for convenience\n",
        "    logging\n",
        ")\n",
        "\n",
        "from datasets import Dataset, DatasetDict # For data handling.\n",
        "\n",
        "from peft import LoraConfig, PeftModel, get_peft_model # PEFT stands for \"Parameter Efficient Fine-Tuning\"\n",
        "                                                       # These objects will help us to run Low Rank Adaptation\n",
        "                                                       # instead of full fine-tuning.\n",
        "\n",
        "torch.manual_seed(42); # Set the state of the random number generator. Important for reproducibility."
      ],
      "metadata": {
        "id": "nAMzy_0FtaUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify GPU Availability\n",
        "\n",
        "If the following cell gives an error, make sure you have a T4 GPU selected in Colab. Go to Runtime -> Change runtime type -> T4 GPU. After that, restart the notebook and re-run the code cells above."
      ],
      "metadata": {
        "id": "9a6fSkXVGFRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not torch.cuda.is_available():\n",
        "    raise ValueError(\"Wrong runtime type, please fix before proceeding. \"\n",
        "                     \"We need a GPU for this fine-tuning notebook to work.\")"
      ],
      "metadata": {
        "id": "0nlr5OqYF3N7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation\n",
        "\n",
        "It's now time to load and tokenize the training and evaluation data."
      ],
      "metadata": {
        "id": "dRi00H10loSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Data\n",
        "\n",
        "The first step is to load training and evaluation data. The `gdown` module provides a way to load data from the internet to the Colab notebook's filesystem.\n",
        "\n",
        "> The URLs point to corresponding URLs for the text on the course page. You can navigate directly to them at [*Men Without Women*](https://uploads.smart.ly/assets/551504549949061acef222a18e665c51dba4bb15c341451942cddeabc6cdcab9/original/551504549949061acef222a18e665c51dba4bb15c341451942cddeabc6cdcab9.txt) and [*The Sun Also Rises*](https://uploads.smart.ly/assets/7f5a282142c700b82ca778a890c74679493ec6651c70e0ee35e84fdef829cccd/original/7f5a282142c700b82ca778a890c74679493ec6651c70e0ee35e84fdef829cccd.txt)."
      ],
      "metadata": {
        "id": "dGf-Wu9UcGUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "gdown.download(\"https://quanticedu.github.io/llm-fine-tuning/MenWithoutWomenCleaned.txt",\n",
        "               \"./MenWithoutWomen.txt\", quiet=True)\n",
        "gdown.download(\"https://quanticedu.github.io/llm-fine-tuning/TheSunAlsoRisesCleaned.txt",\n",
        "               \"./TheSunAlsoRises.txt\", quiet=True)\n",
        "\n",
        "# Loading our training data (one book)\n",
        "with open(\"MenWithoutWomen.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_training_text = f.read()\n",
        "\n",
        "# Loading our evaluation and test data (one book)\n",
        "with open(\"TheSunAlsoRises.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "    raw_eval_text = f.read()"
      ],
      "metadata": {
        "id": "25qEWejAv8gQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get a Tokenizer\n",
        "\n",
        "The next step is to tokenize the training and evaluation data. To begin, we get the pretrained tokenizer from the pretrained model. Run this cell, ignoring warnings about `HF_TOKEN` and special tokens."
      ],
      "metadata": {
        "id": "uTTrs1BXIpex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The model that you want to train from the Hugging Face hub\n",
        "model_name = \"microsoft/phi-2\"\n",
        "revision = \"523a3d62e793d3f51ad6334ccfd3b67de28771c0\"\n",
        "# Once you get a model working, it's good practice to \"freeze\" the revision so\n",
        "# subsequent changes to the model don't affect your results. To do this on Hugging Face,\n",
        "# go to the model's page, select \"Files and Versions\", then \"History\" in the upper\n",
        "# right. The select the Copy icon next to the commit hash of the most recent\n",
        "# commit and use the full hash as the revision parameter for loading the tokenizer\n",
        "# and the model itself.\n",
        "\n",
        "# Load the pre-trained Phi2 tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, revision=revision)\n",
        "tokenizer.pad_token = tokenizer.eos_token # A common, slightly hacky solution.\n",
        "                                          # Some models are trained without padding,\n",
        "                                          # but we can usually reuse the eos (end of sequence) token\n",
        "                                          # for padding purposes."
      ],
      "metadata": {
        "id": "giOj9T_OSuNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore Tokenizing Strategies\n",
        "\n",
        "We can use this to explore different strategies for tokenizing the data. Try various combinations of `True` and `False` for the `truncation` and `return_overflowing_tokens` parameters to see how the tokenizer behaves."
      ],
      "metadata": {
        "id": "shq3BPhpmXLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying our pre-trained tokenizer to text.\n",
        "outputs = tokenizer(\n",
        "    raw_training_text,\n",
        "    truncation=False,\n",
        "    max_length=128,\n",
        "    return_overflowing_tokens=False,\n",
        "    return_length=True,\n",
        ")\n",
        "\n",
        "print(type(outputs))\n",
        "for key in outputs:\n",
        "  print(f\"{key}:\")\n",
        "  if len(outputs[key]) > 1:\n",
        "    print(f\"  length: {len(outputs[key])}\\n  first three elements: {outputs[key][:3]}\\n  last three elements = {outputs[key][-3:]}\")\n",
        "  else:\n",
        "    print(f\"  value: {outputs[key][0]}\")"
      ],
      "metadata": {
        "id": "rP-p0QMr03v5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize the Datasets\n",
        "\n",
        "We now write the code to tokenize the datasets. Here we'll select `True` for `truncation` and `return_overflowing_tokens`. We'll also add parameters to control the padding and select what type of data will be returned."
      ],
      "metadata": {
        "id": "q9TiaGRAaISt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train_data = Dataset.from_dict({\"text\": [raw_training_text]}) # Wrapping our data into a huggingface library's Dataset object,\n",
        "raw_eval_data = Dataset.from_dict({\"text\": [raw_eval_text]})      # which allows convenient data preprocessing options.\n",
        "\n",
        "raw_datasets = DatasetDict( # Wrapping both datasets into a \"DatasetDict\" object that can hold different data splits.\n",
        "    {\n",
        "        \"train\": raw_train_data,\n",
        "        \"valid\": raw_eval_data,\n",
        "    }\n",
        ")\n",
        "\n",
        "raw_datasets.set_format(\"torch\") # Makes the datasets more convenient to use with pytorch.\n",
        "\n",
        "# How much context should we consider at once. We'll set it to a relatively short 250 token context to keep things manageable.\n",
        "context_length = 250\n",
        "\n",
        "def tokenize(element):\n",
        "    '''A function to tokenize a given element(or a batch of elements) in the data.'''\n",
        "\n",
        "    outputs = tokenizer(\n",
        "        element[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=context_length,\n",
        "        return_overflowing_tokens=True,\n",
        "        padding=True, # Using a special padding token, extend shorter sequences in the batch of elements to match the length of the longest one.\n",
        "        return_tensors='pt' # Returned data will be PyTorch tensors.\n",
        "    )\n",
        "\n",
        "    tokenized_words = outputs[\"input_ids\"].to(\"cuda:0\")\n",
        "\n",
        "    # Note that in Causal Language Modeling, the answer to each input is just the next token in the input.\n",
        "    # So essentially the outputs are inputs shifted by one. Here we provide labels to be the same as inputs\n",
        "    # because during training, this label shifting will be done for us automatically.\n",
        "    return {\"input_ids\": tokenized_words, \"labels\": tokenized_words.clone()}\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize, remove_columns=\"text\"\n",
        ").shuffle()\n"
      ],
      "metadata": {
        "id": "9whHGV6AVR1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Specify Quantization and Load the Model\n",
        "\n",
        "We'll come back to the tokenized datasets in the next lesson. Now we turn our attention to loading the model."
      ],
      "metadata": {
        "id": "Q4hmQn69QZHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select Quantization\n",
        "\n",
        "`BitsandBytes` is a Python library used for quantizing models which we'll use to quantize our model. In the cell below, we specify the quantization parameters."
      ],
      "metadata": {
        "id": "i_AE9XLV2GBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# bitsandbytes (quantization) parameters\n",
        "################################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = False # Whether to quantize model weights to 4bits.\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\" # For some GPUs, 'bfloat16' format could be the optimal choice.\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"fp4\" # Choosing between different number representation formats.\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = True\n",
        "\n",
        "# Use variables above to define a quantization configuration object.\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        "\n",
        ")\n"
      ],
      "metadata": {
        "id": "ib_We3NLtj2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Model\n",
        "\n",
        "Load the model using parameters specified above."
      ],
      "metadata": {
        "id": "Eh_sHgrHxzBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify that we want to load the entire model on the GPU 0\n",
        "if not torch.cuda.is_available():\n",
        "    raise ValueError(\"Please make sure your runtime is set to GPU.\")\n",
        "\n",
        "device = \"cuda:0\" # The first among the available GPUs.\n",
        "device_map = {\"\": 0} # Specify which elements of the model go to which device.\n",
        "                     # This is especially relevant for huge models that don't fit on one GPU.\n",
        "                     # In our case, we map everything to device 0 (GPU number 0) when loading the model.\n",
        "\n",
        "\n",
        "# Load base model\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    revision=revision,\n",
        "    device_map=device_map,\n",
        "    trust_remote_code=True, # This is to let huggingface know that we are downloading this custom model from a trusted source.\n",
        "    quantization_config=bnb_config if use_4bit else None,\n",
        "    torch_dtype=torch.float16 # When quantization is not used,\n",
        "                              # we need to specify this to avoid loading the model in 32bit.\n",
        ")\n",
        "\n",
        "model.config.use_cache = False # Caching speeds up inference, but is irrelevant for training/fine-tuning.\n",
        "                               # We've found it interferes with Colab behavior when different models are loaded/unloaded.\n",
        "                               # So we'll keep it off. In practice, for inference, setting it to True (default) is advisable."
      ],
      "metadata": {
        "id": "OJXpOgBFuSrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before jumping to fine-tuning it is crucial to check that the base model works as expected. We'll also use this opportunity to quickly check whether quantization affects performance. In this lesson, we'll make a simple manual comparison.\n",
        "\n",
        "There is a [popular myth](https://theamericanscholar.org/the-shortest-story-ever-told/) that Hemingway once won a bet by writing a one-sentence story that made people cry. Therefore we'll use the following prompt in our examples: \"As promised, here is a one-sentence story that will make you cry: \""
      ],
      "metadata": {
        "id": "VvDxw3UIbhtL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize our prompt:\n",
        "inputs = tokenizer('''As promised, here is a one-sentence story that will make you cry: ''',\n",
        "                      return_tensors=\"pt\").to(device)\n",
        "\n",
        "torch.manual_seed(42) # specify the seed for the (pseudo)random number generator:\n",
        "                      # useful when we use sampling something randomly but want our results to be reproducible.\n",
        "\n",
        "with torch.no_grad(): # Currently we are not training our model, so we don't need to keep track of gradients.\n",
        "\n",
        "    generation_config = GenerationConfig(max_length=200,\n",
        "                                         eos_token_id=tokenizer.eos_token_id,\n",
        "                                         do_sample=False,    # Whether to use deterministic (highest probability) decoding\n",
        "                                         use_cache=False)    # or sample each next word proportionally to its predicted probability.\n",
        "\n",
        "    outputs = model.generate(**inputs, generation_config=generation_config)\n",
        "\n",
        "    text = tokenizer.batch_decode(outputs)[0]\n",
        "    print(text)\n"
      ],
      "metadata": {
        "id": "R5uL8pXnkzAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bonus exploration suggestions:** When sampling, you can play around with extra parameters to change sampling procedure and outcomes. Try changing `do_sample` to `True` and adding some of the following arguments to the generation config:\n",
        "\n",
        "*   temperature=1.2 (the higher the temperature-the more creative/unhinged the generation will be)\n",
        "*   top_k=50 (sampling is restricted to top 50 most likely words, to have some creativity without going too crazy)\n",
        "* top_p=0.7 (nucleus sampling - similar idea, but each next word is drawn out of a selection of the most probable words that together have a probability of 70%)."
      ],
      "metadata": {
        "id": "QXOiEUHGlAzd"
      }
    }
  ]
}
