{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quanticedu/llm-fine-tuning/blob/main/PEFT_with_LoRA_and_QLoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameter-Efficient Fine-Tuning (PEFT) with Low-Rank Adaptation (LoRA) and Quantized Low-Rank Adaptation (QLoRA)\n",
        "\n",
        "In this Colab notebook, you'll determine the hyperparameters you'll need to fine-tune the Phi-2 model using the PEFT strategies of LoRA and QLoRA.\n",
        "\n",
        "> This notebook is based on [@maximelabonne's LLama2 fine-tuning notebook](https://github.com/mlabonne/llm-course/blob/main/Fine_tune_Llama_2_in_Google_Colab.ipynb), which is, in turn, based on Younes Belkada's [GitHub Gist](https://gist.github.com/younesbelkada/9f7f75c94bdc1981c8ca5cc937d4a4da). It also borrows from [this example](https://github.com/brevdev/notebooks/blob/main/phi2-finetune-own-data.ipynb) on phi2 fine-tuning."
      ],
      "metadata": {
        "id": "OSHlAbqzDFDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Tokenize the Training Data\n",
        "\n",
        "These two cells contain all the code from the previous lesson. The first cell installs the needed dependencies and tokenizes the training datasets. The second loads the model unquantized (you'll reload it quantized later in the lesson). Refer to the previous lesson if you need a refresher on anything here.\n",
        "\n",
        "Select the T4 GPU runtime and run the two cells.\n"
      ],
      "metadata": {
        "id": "8PgSgS6YVKJc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLXwJqbjtPho"
      },
      "outputs": [],
      "source": [
        "# Install and import needed libraries\n",
        "!pip install accelerate==0.26.1 bitsandbytes==0.42.0 datasets==2.14.6 dill==0.3.7 einops==0.4.1 fsspec==2023.10.0 multiprocess==0.70.15 nvidia-cublas-cu12==12.1.3.1 nvidia-cuda-cupti-cu12==12.1.105 nvidia-cuda-nvrtc-cu12==12.1.105 nvidia-cuda-runtime-cu12==12.1.105 nvidia-cudnn-cu12==8.9.2.26 nvidia-cufft-cu12==11.0.2.54 nvidia-curand-cu12==10.3.2.106 nvidia-cusolver-cu12==11.4.5.107 nvidia-cusparse-cu12==12.1.0.106 nvidia-nccl-cu12==2.18.1 nvidia-nvtx-cu12==12.1.105 peft==0.7.1 tokenizers==0.15.2 torch==2.1.0 transformers==4.36.2 triton==2.1.0 trl==0.7.0 xxhash==3.3.0\n",
        "\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    raise ValueError(\"Wrong runtime type, please fix before proceeding. \"\n",
        "                     \"We need a GPU for this fine-tuning notebook to work.\")\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, # Will be used to load the pre-trained model\n",
        "    AutoTokenizer, # Will be used to load the pre-trained tokenizer\n",
        "    BitsAndBytesConfig, # For model quantization settings\n",
        "    GenerationConfig, # To control generation (inference) from a model\n",
        "    TrainingArguments, # To specify parameters of the fine-tuning process\n",
        "    Trainer, # The object that abstracts away the training and evaluation loop\n",
        "    pipeline, # Stringing together tokenization and inference, for convenience\n",
        "    logging\n",
        ")\n",
        "\n",
        "from datasets import Dataset, DatasetDict # For data handling.\n",
        "\n",
        "from peft import LoraConfig, PeftModel, get_peft_model # PEFT stands for \"Parameter Efficient Fine-Tuning\"\n",
        "                                                       # These objects will help us to run Low Rank Adaptation\n",
        "                                                       # instead of full fine-tuning.\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42); # Set the state of the random number generator. Important for reproducibility.\n",
        "\n",
        "#Load and tokenize the training data\n",
        "import gdown # For downloads from google drive\n",
        "\n",
        "url_the_sun = 'https://quanticedu.github.io/llm-fine-tuning/TheSunAlsoRisesCleaned.txt'\n",
        "gdown.download(url_the_sun, \"./TheSunAlsoRisesCleaned.txt\", quiet=True)\n",
        "\n",
        "url_men_without = 'https://quanticedu.github.io/llm-fine-tuning/MenWithoutWomenCleaned.txt'\n",
        "gdown.download(url_men_without, \"./MenWithoutWomenCleaned.txt\", quiet=True)\n",
        "\n",
        "with open(\"MenWithoutWomenCleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_training_text = f.read()\n",
        "\n",
        "with open(\"TheSunAlsoRisesCleaned.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "    raw_eval_text = f.read()\n",
        "\n",
        "# The model that you want to train from the Hugging Face hub\n",
        "model_name = \"microsoft/phi-2\"\n",
        "revision = \"523a3d62e793d3f51ad6334ccfd3b67de28771c0\"\n",
        "\n",
        "# Load the pre-trained Phi2 tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, revision=revision)\n",
        "tokenizer.pad_token = tokenizer.eos_token # A common, slightly hacky solution.\n",
        "                                          # Some models are trained without padding,\n",
        "                                          # but we can usually reuse the eos (end of sequence) token\n",
        "                                          # for padding purposes.\n",
        "\n",
        "# Note: 'special tokens' warning can be ignored, as well as the warning about the \"HF_TOKEN\"\n",
        "\n",
        "raw_train_data = Dataset.from_dict({\"text\": [raw_training_text]}) # Wrapping our data into a huggingface library's Dataset object,\n",
        "raw_eval_data = Dataset.from_dict({\"text\": [raw_eval_text]})      # which allows convenient data preprocessing options.\n",
        "\n",
        "raw_datasets = DatasetDict( # Wrapping both datasets into a \"DatasetDict\" object that can hold different data splits.\n",
        "    {\n",
        "        \"train\": raw_train_data,\n",
        "        \"valid\": raw_eval_data,\n",
        "    }\n",
        ")\n",
        "\n",
        "raw_datasets.set_format(\"torch\") # Makes the datasets more convenient to use with pytorch.\n",
        "\n",
        "# How much context should we consider at once. We'll keep it low to keep things manageable.\n",
        "context_length = 250\n",
        "\n",
        "def tokenize(element):\n",
        "    '''A function to tokenize a given element(or a batch of elements) in the data.'''\n",
        "\n",
        "    outputs = tokenizer(\n",
        "        element[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=context_length,\n",
        "        return_overflowing_tokens=True,\n",
        "        padding=True, # Using a special padding token, extend shorter sequences in the batch of elements to match the length of the longest one.\n",
        "        return_tensors='pt' # Returned data will be PyTorch tensors.\n",
        "    )\n",
        "\n",
        "    tokenized_words = outputs[\"input_ids\"].to(\"cuda:0\")\n",
        "\n",
        "    # Note that in Causal Language Modeling, the answer to each input is just the next token in the input.\n",
        "    # So essentially the outputs are inputs shifted by one. Here we provide labels to be the same as inputs\n",
        "    # because during training, this label shifting will be done for us automatically.\n",
        "    return {\"input_ids\": tokenized_words, \"labels\": tokenized_words.clone()}\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize, batched=True, remove_columns=\"text\"\n",
        ").shuffle()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify quantization and load the model\n",
        "################################################################################\n",
        "# bitsandbytes (quantization) parameters\n",
        "################################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = False # Whether to quantize model weights to 4bits (QLoRA).\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\" # For some GPUs, 'bfloat16' format could be the optimal choice.\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\" # Choosing between different number representation formats.\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False\n",
        "\n",
        "# Use variables above to define a quantization configuration object.\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        "\n",
        ")\n",
        "\n",
        "device = \"cuda:0\" # The first among the available GPUs.\n",
        "device_map = {\"\": 0} # Specify which elements of the model go to which device.\n",
        "                     # This is especially relevant for huge models that don't fit on one GPU.\n",
        "                     # In our case, we map everything to device 0 (GPU number 0) when loading the model.\n",
        "\n",
        "# Load base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    revision=revision,\n",
        "    device_map=device_map,\n",
        "    trust_remote_code=True, # This is to let huggingface know that we are downloading this custom model from a trusted source.\n",
        "    quantization_config=bnb_config if use_4bit else None,\n",
        "    torch_dtype=torch.float16 # When quantization is not used,\n",
        "                              # we need to specify this to avoid loading the model in 32bit.\n",
        ")\n",
        "\n",
        "model.config.use_cache = False # Caching speeds up inference, but is irrelevant for training/fine-tuning.\n",
        "                               # We've found it interfere with Colab behavior when different models are loaded/unloaded.\n",
        "                               # So we'll keep it off. In practice, for inference, setting it to True (default) is advisable."
      ],
      "metadata": {
        "id": "_V0xffFW8jKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base Model Samples\n",
        "\n",
        "Before we fine-tune the model, we should get samples of its baseline performance. First we create a pipeline for convenience.\n"
      ],
      "metadata": {
        "id": "VvDxw3UIbhtL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "VsEFtZu-6J76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we create two samples. In the first we'll use the same prompt we used in the previous lesson. Note that in the previous lesson we had `do_sample=False` in the generation configuration, which means that this output will not be the same."
      ],
      "metadata": {
        "id": "CnFWKjPy8Mlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = GenerationConfig(max_length=200,\n",
        "                                      do_sample=True,    # Whether to use deterministic (highest probability) decoding\n",
        "                                      use_cache=False,    # or sample each next word proportionally to its predicted probability.\n",
        "                                      temperature=1,\n",
        "                                      eos_token_id=tokenizer.eos_token_id,\n",
        "                                      bos_token_id=tokenizer.eos_token_id,\n",
        "                                      pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "# Try the old \"sad one-sentence story\" prompt we used in the previous lesson:\n",
        "torch.manual_seed(42)\n",
        "result = pipe(\"As promised, here is a one-sentence story that will make you cry: \", generation_config=generation_config)\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "_SfFlJDaI1p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the second sample, we'll have the model give us a continuation for an opening of a story."
      ],
      "metadata": {
        "id": "Ztz9OhJK9Q4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue a generic story opening:\n",
        "torch.manual_seed(42)\n",
        "result = pipe(\"I went outside,\", generation_config=generation_config)\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "sV8J1fh-I235"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Trainable Modules\n",
        "\n",
        "To determine which modules to apply LoRA to, we need to know which modules are in the model."
      ],
      "metadata": {
        "id": "yRdHDeShPtGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "SOl-qW2kP2qK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training Hyperparameters\n",
        "\n",
        "This cell sets the hyperparameters and creates a PEFT model from the base model."
      ],
      "metadata": {
        "id": "eXcyL_Ec_vea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Fine-tuned model name for saving later\n",
        "new_model_name = \"phi2-hemingway\"\n",
        "\n",
        "################################################################################\n",
        "# LoRA parameters\n",
        "################################################################################\n",
        "\n",
        "# LoRA attention dimension\n",
        "# 1 is the minimum, which would result in extremely limited flexibility.\n",
        "# The higher the number - the more flexible our LoRA.\n",
        "# adjustment matrices will be. The cost is higher memory demand and longer training.\n",
        "# If we increase it too much, we'll essentially be doing full fine-tuning on the\n",
        "# weights to which LoRA is applied (see \"training_modules\" parameter in the next cell).\n",
        "# Common values to try are: 8, 16, 32.\n",
        "lora_r = 32\n",
        "\n",
        "# Alpha parameter for LoRA scaling # Covered in the lesson directly\n",
        "# Higher alpha will result in higher impact of lora adaptation.\n",
        "# A common rule of thumb is to set this to lora_r times two.\n",
        "# But it's not guaranteed to be best and experimentation can help find more optimal values.\n",
        "lora_alpha = 64\n",
        "\n",
        "# Dropout probability for LoRA layers\n",
        "# Dropout refers to randomly \"switching off\" a certain proportion of neurons.\n",
        "# This encourages the network not to rely on any one weight too much and thus be\n",
        "# more robust.\n",
        "lora_dropout = 0.1\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# TrainingArguments parameters\n",
        "################################################################################\n",
        "\n",
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# Number of training epochs. How many times to go over the dataset.\n",
        "# Overly high - increased risk of overfitting (memorizing the training set without understanding)\n",
        "# Overly low - increased risk of cutting training too early.\n",
        "# Reasonable value can be selected by selecting a large value and monitoring validation set performance.\n",
        "\n",
        "num_train_epochs = 3.0\n",
        "\n",
        "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "# Can speed up training and decrease memory demands by\n",
        "# using different quantization levels on different network parts.\n",
        "# Important for QLoRA. Might not work on some/many GPUs.\n",
        "fp16 = True if use_4bit else False\n",
        "bf16 = False\n",
        "\n",
        "# Batch siz (how many training examples to work with in parallel) per GPU for training\n",
        "# Usually, the higher the batch size - the better (results in more stable learning).\n",
        "# BUT go too high - and you'll quickly run out of GPU memory.\n",
        "# Generally, select the highest number you can without running out of memory.\n",
        "per_device_train_batch_size = 2\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "# This can often be a bit higher since during evaluation we don't need to store gradients.\n",
        "# The higher this number - the faster the evaluation will be.\n",
        "per_device_eval_batch_size = 4\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "# If you batch size is small, you can increase this number for more stable training.\n",
        "# (we'll accumulate evidence for some time before making the weight update step)\n",
        "# It's essentially the same as batch size, but done sequentially instead of in parallel.\n",
        "gradient_accumulation_steps = 2\n",
        "\n",
        "# Enable gradient checkpointing. Lowers memory demand by clever combination\n",
        "# of caching and recomputation. The cost is a small slowdown.\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Maximum gradient normal (gradient clipping). Prevent gradients from growing too large\n",
        "# and causing training instabilities or numerical overflows.\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "# Weight decay prevents individual weights from becoming too large.\n",
        "# This is a classical way of softly reducing model flexibility / degrees of freedom.\n",
        "# If weight decay is too high, all weights will be incentivised to become near-zero.\n",
        "weight_decay = 0.000 # 0.001, 0.005, 0.0001 are all values one might want to try.\n",
        "                     # Be careful with this parameter, though, as too much weight decay\n",
        "                     # might make the model forget everything.\n",
        "\n",
        "\n",
        "# Optimizer to use (intuitively, the training data will tell us the direction\n",
        "# on how much each weight should be changed to improve the performance a little.\n",
        "# But the optimizer will 'decide' how exactly to use this information: change\n",
        "# fast or slow, with or without inertia, etc.)\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 1e-4 # How fast to step along the directions described above.\n",
        "                     # AdamW is adaptive, meaning that it will internally adjust this,\n",
        "                     # but it's still important to choose an adequate starting point.\n",
        "\n",
        "# Learning rate schedule. Learning rate additional changes during training according to a pre-specified\n",
        "# schedule. Usually, getting smaller towards the end of training, with the idea that\n",
        "# towards the end, we are making finer adjustments than in the beginning.\n",
        "# A nice article covering different scheduler shapes: https://towardsdatascience.com/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863\n",
        "# The scheduler is especially important if we were to use the SGD optimizer.\n",
        "lr_scheduler_type = \"cosine\"\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = -1\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "# Warm-up refers to starting, in contrast, with a lower learning rate, to avoid\n",
        "# overly dramatic changes in the very beginning of learning.\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 0\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 25\n",
        "\n",
        "# Define LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    target_modules=[ # Which model parts to apply L matrices to.\n",
        "        \"fc1\",      # use print(model) to make a more informed decision.\n",
        "        \"fc2\",       # Weights related to queries, keys, and values are a must\n",
        "        \"k_proj\",\n",
        "        \"q_proj\",\n",
        "        \"v_proj\",\n",
        "        \"dense\"\n",
        "    ],\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"lora_only\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model_peft = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "id": "ECdKCng-zLCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's investigate our PEFT model."
      ],
      "metadata": {
        "id": "GbaXMHdYIoif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_peft)\n",
        "\n",
        "## From https://github.com/brevdev/notebooks/blob/main/phi2-finetune-own-data.ipynb\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in a model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "print_trainable_parameters(model_peft)"
      ],
      "metadata": {
        "id": "uWRMat7GKJCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fine-Tuning\n",
        "\n",
        "In this cell we'll create the trainer object, which uses the training and evaluation datasets along with the PEFT model and arguments to control the training. Before we actually fine tune, we'll look at the evaluation loss of the pre-trained model on the new evaluation data."
      ],
      "metadata": {
        "id": "pt1FamFdnAo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=25,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model_peft,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets[\"valid\"],\n",
        "    args=training_arguments,\n",
        ")\n",
        "\n",
        "trainer.evaluate() # To see the loss before the start of training.\n"
      ],
      "metadata": {
        "id": "QZNU9eSKMJcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we conduct the actual fine tuning. This process will take a few minutes."
      ],
      "metadata": {
        "id": "8Vib0kCi33DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "# Train model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "mTkVv6F98lum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how important it is to evaluate the model at least once before training. Without it it might seem that the loss barely changed compared to the non-fine-tuned model. Often the biggest improvement happens before the first evaluation."
      ],
      "metadata": {
        "id": "5gz5ipdCRgPH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post-Training Samples"
      ],
      "metadata": {
        "id": "QUBGbNQvZRuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logging.set_verbosity(logging.CRITICAL) # Ignore warnings\n",
        "trainer.model.eval(); # Set the model into evaluation regime.\n",
        "pipe = pipeline(task=\"text-generation\", model=trainer.model, tokenizer=tokenizer)\n",
        "generation_config = GenerationConfig(max_length=200,\n",
        "                                      do_sample=True,    # Whether to use deterministic (highest probability) decoding\n",
        "                                      use_cache=False,    # or sample each next word proportionally to its predicted probability.\n",
        "                                      temperature=1,\n",
        "                                      eos_token_id=tokenizer.eos_token_id,\n",
        "                                      bos_token_id=tokenizer.eos_token_id,\n",
        "                                      pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "# Sad one-sentence story completion\n",
        "torch.manual_seed(42)\n",
        "result = pipe(\"As promised, here is a one-sentence story that will make you cry: \", generation_config=generation_config)\n",
        "print(result[0]['generated_text'])\n"
      ],
      "metadata": {
        "id": "-0RMJuL-7e-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generic story beginning completion:\n",
        "torch.manual_seed(42)\n",
        "prompt = \"I went outside,\"\n",
        "result = pipe(prompt, generation_config=generation_config)\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "dCGvSt9m0OOs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
